<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="/assets/main.css">
    <title>Minimalist Blog</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
        }
        h1 {
            color: #1a1a1a;
            border-bottom: 2px solid #eaeaea;
            padding-bottom: 10px;
            text-align: center;
        }
        .comic-container {
            text-align: center;
            margin: 20px 0 30px 0;
        }
        .comic-image {
            max-width: 100%;
            height: auto;
            border-radius: 5px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        }
        ol {
            margin-bottom: 20px;
        }
        p {
            margin-bottom: 16px;
        }
        .emphasis {
            font-style: italic;
        }
        .conclusion {
            background-color: #f9f9f9;
            border-left: 4px solid #ccc;
            padding: 10px 15px;
            margin: 20px 0;
        }
    </style>
</head>
<body>
    <header>
        <a href="index.html">&larr; Back</a>
    </header>
    <main style="max-width: 800px; margin: 0 auto; padding: 0 15px;">
    <h1 style="font-size: 2rem;">Half-baked thoughts: On Benchmarks</h1>
    <div class="comic-container">
        <!-- Replace "your-comic-image.jpg" with your actual image filename -->
        <img src="xkcd.jpg" alt="Comic about AI expectations" class="comic-image">
    </div>
    <p><em>AKA “Why is our model not able to use the right tool for our particular use case, when it is able to solve SAT math questions?” </em></p>
    

    <p>While new benchmarks are being reached by models, this does not always translate to gains in business-relevant products and tasks. A couple of issues with these benchmarks:</p>
    
    <ul>
        <li>They're easy to hack.</li>
        <li>They're usually in multiple choice.</li>
    </ul>
    <p>Now, multiple-choice isn't inherently problematic. However, most real-world tasks aren't multiple-choice, and research has shown that models trained in this format on certain types of knowledge don't necessarily know how to apply that knowledge in open-ended, real-world scenarios.  </p>
    
    <p>Think of benchmarks as being directionally indicative of reasoning ability, but you need to set up your own evaluations.</p>
    
    <p>For example, if you're working on agents, you need to make sure:</p>
    
    <ol>
        <li> The agent doesn't perform any unnecessary steps.</li>
        <li>It completes the task successfully.</li>
        <li>The task given to it is feasible to be done with the context provided.</li>
    </ol>
    <p>Often times, AI benchmarks don't check for one of the above. For exxample, <a href="https://arxiv.org/abs/2406.12045">tau-bench</a> doesn't check for the first, and <a href="https://openai.com/index/introducing-swe-bench-verified/">SWE-bench didn't check for the third.</a></p>
    
    <p>Of course, AI is definitely getting smarter - but just know, just because models increase in numbers out of the box doesn't mean there will be a lot of prompt tuning and scaffolding to get it to work for your usecase.</p>
</main>
</body>
</html>